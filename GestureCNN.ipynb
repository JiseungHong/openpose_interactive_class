{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "GestureCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwEdGS-uXtZs"
      },
      "source": [
        "# Hand Gesture Detector with CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXZb8liYgxww"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "root = '/gdrive/My Drive/MotionDetection'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x9mGVUZX2mB"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RLdxMOfwInz"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import sys\n",
        "\n",
        "import numpy\n",
        "import pickle\n",
        "import torch\n",
        "import itertools\n",
        "import time\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50zR64OZYLWr"
      },
      "source": [
        "## Functions for Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFCFtxSdhU2z"
      },
      "source": [
        "def normalize(tens):\n",
        "  \"\"\" Normalize 2 dimensional Tensor: number of joints * xy-coordinate\n",
        "      Maintain the ratio and make sure the joints are in 100*100 box.\"\"\"\n",
        "  max_x = max(tens[:, 0])\n",
        "  min_x = min(tens[:, 0])\n",
        "\n",
        "  max_y = max(tens[:, 1])\n",
        "  min_y = min(tens[:, 1])\n",
        "\n",
        "  tens[:, 0] = (tens[:, 0] - min_x + 1) / (max_y - min_y) * 100\n",
        "  tens[:, 1] = (tens[:, 1] - min_y + 1) / (max_y - min_y) * 100\n",
        "\n",
        "  return tens\n",
        "\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# test1 = torch.rand(1, 2, 21, 2)\n",
        "# test1[:, :, :, 0] = 12 * test1[:, :, :, 0]\n",
        "# test1[:, :, :, 1] = 10 * test1[:, :, :, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxdasWqThWbY"
      },
      "source": [
        "def randn_ordered(tens, n):\n",
        "  \"\"\"Random sample n from a tensor, maintaining the order\"\"\"\n",
        "  assert tens.size(0) > n\n",
        "  print(type(tens.shape[0]))\n",
        "  r = random.sample(range(tens.shape[0]), n)\n",
        "  r.sort()\n",
        "  \n",
        "  return tens[r]\n",
        "  \n",
        "# random.seed(a=123)\n",
        "# test1 = torch.arange(3*4*5)\n",
        "# test1 = test1.reshape(5, 4, 3)\n",
        "# \n",
        "# # print(test1)\n",
        "# # print(randn_ordered(test1, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-j4EqVnhX8c"
      },
      "source": [
        "def cut_n(tens, n):\n",
        "  \"\"\"cut n frames in the middle(0.6) from the sequence\"\"\"\n",
        "  if tens.shape[0] == n:\n",
        "    return tens\n",
        "\n",
        "  tt = int(tens.shape[0] * 0.6) - 1\n",
        "  c = tens[tt-n//2:tt+(n-n//2)]\n",
        "  if c.shape[0] != n:\n",
        "    return None\n",
        "  return tens[tt-n//2:tt+(n-n//2)]\n",
        "\n",
        "# test1 = torch.arange(10*5*3)\n",
        "# test1 = test1.reshape(10, 5, 3)\n",
        "# \n",
        "# print(test1)\n",
        "# print(cut_n(test1, 2))\n",
        "# test2= torch.tensor([[5.8782611e+02, 5.1130432e+02, 3.5325505e-02],  [5.8304352e+02, 4.3956522e+02, 2.4411943e-02],  [4.8260870e+02, 5.2804346e+02, 2.0107470e-02]])\n",
        "# print(cut_n(test2,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdrre36eh8ul"
      },
      "source": [
        "def cut_max2(tens):\n",
        "  \"\"\" Find maximum confidence frame and return two frames(maximum and one of its neighbor)\"\"\"\n",
        "  # print(torch.sum(tens[:, :, -1], dim=1))\n",
        "  tt = torch.argmax(torch.sum(tens[:, :, -1], dim=1), dim=0)\n",
        "  # return torch.unsqueeze(tens[tt], 0)\n",
        "  # print(tt)\n",
        "  if tt == 0:\n",
        "    return tens[:2]\n",
        "  if tt == tens.shape[0]-1:\n",
        "    return tens[-2:]\n",
        "  if torch.sum(tens[tt-1, :, -1]) >= torch.sum(tens[tt+1, :, -1]):\n",
        "    return tens[tt-1:tt+1]\n",
        "  return tens[tt:tt+2]\n",
        "\n",
        "# test1 = torch.rand(5*21*3)\n",
        "# test1 = test1.reshape(5, 21, 3)\n",
        "\n",
        "\n",
        "# print(test1)\n",
        "# print(cut_max2(test1))\n",
        "# test2= torch.tensor([[5.8782611e+02, 5.1130432e+02, 3.5325505e-02],  [5.8304352e+02, 4.3956522e+02, 2.4411943e-02],  [4.8260870e+02, 5.2804346e+02, 2.0107470e-02]])\n",
        "# print(cut_max2(test2,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZSJQhath-Ws"
      },
      "source": [
        "def cut_max3(tens):\n",
        "  \"\"\" Find maximum confidence frame and return two frames(maximum and two of its neighbor)\"\"\"\n",
        "  # print(torch.sum(tens[:, :, -1], dim=1))\n",
        "  tt = torch.argmax(torch.sum(tens[:, :, -1], dim=1), dim=0)\n",
        "  if tt == 0:\n",
        "    return tens[:3]\n",
        "  if tt == tens.shape[0]-1:\n",
        "    return tens[-3:]\n",
        "  return tens[tt-1:tt+2]\n",
        "# test = torch.rand(5*21*3)\n",
        "# test = test.reshape(5, 21, 3)\n",
        "\n",
        "\n",
        "# print(test)\n",
        "# print(cut_max2(test))\n",
        "# test2= torch.tensor([[5.8782611e+02, 5.1130432e+02, 3.5325505e-02],  [5.8304352e+02, 4.3956522e+02, 2.4411943e-02],  [4.8260870e+02, 5.2804346e+02, 2.0107470e-02]])\n",
        "# print(cut_max2(test2,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKlSVa2uf1V-"
      },
      "source": [
        "def resample_n(tens, n):\n",
        "  \"\"\" Resample n frames from a tensor\n",
        "      Example) \"1, 2, 3, 4, 5\", 13 --> \"1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5\" \"\"\"\n",
        "  # origin_len = tens.shape[0]\n",
        "  origin_len = len(tens)\n",
        "  quo = n // origin_len\n",
        "  rem = origin_len - n % origin_len\n",
        "  tar_l = [i for i in range(rem // 2 + rem % 2)]\n",
        "  tar_r = [origin_len - i-1 for i in range(rem // 2)]\n",
        "  tar = tar_l+tar_r\n",
        "  result = []\n",
        "  for i in range(origin_len):\n",
        "    if i in tar:\n",
        "      result += [tens[i]] * quo\n",
        "    else:\n",
        "      result += [tens[i]] * (quo+1)\n",
        "  result = torch.stack(result, dim = 0)\n",
        "  return result\n",
        "\n",
        "# test1 = torch.arange(13*2*2)\n",
        "# test1 = test1.reshape(13, 2, 2)\n",
        "# print(test1)\n",
        "# print(resample_n(test1, 30))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehznEjprYOuD"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJjbr22xhake"
      },
      "source": [
        "def parser(line):\n",
        "  \"\"\" Our data is in csv format. Parse each line of the csv\"\"\"\n",
        "  id, left, right = line.split('[[[')\n",
        "  id = id[:-2]\n",
        "  left = '[' + left[:-4]\n",
        "  right = '[' + right[:-2]\n",
        "\n",
        "  left = left.split(',  ')\n",
        "  left = [i.strip('[]').split(' ') for i in left]\n",
        "  left = [list(filter(None, leftelem)) for leftelem in left]\n",
        "  left = [float(i) for j in left for i in j]\n",
        "  left = torch.tensor(left)\n",
        "  left = left.reshape((-1, 3))\n",
        "  left_avg = sum(left[:, 2])/21\n",
        "  \n",
        "  right = right.split(',  ')\n",
        "  right = [i.strip('[]').split(' ') for i in right]\n",
        "  right = [list(filter(None, rightelem)) for rightelem in right]\n",
        "  right = [float(i) for j in right for i in j]\n",
        "  right = torch.tensor(right)\n",
        "  right = left.reshape((-1, 3))\n",
        "  right_avg = sum(right[:, 2])/21\n",
        "\n",
        "  v_id, img_id = id.split('_')\n",
        "\n",
        "  if left_avg >= right_avg:\n",
        "    left[:,0] = -left[:,0]\n",
        "    return v_id, img_id, normalize(left)\n",
        "    # return v_id, img_id, left\n",
        "  return v_id, img_id, normalize(right)\n",
        "  # return v_id, img_id, right\n",
        "  #   return v_id, img_id, normalize(left[:, :2])\n",
        "  # return v_id, img_id, normalize(right[:, :2])\n",
        "\n",
        "\n",
        "# v_id, img_id, hand = parser('1_00001,\"[[[5.8782611e+02 5.1130432e+02 3.5325505e-02],  [5.8304352e+02 4.3956522e+02 2.4411943e-02],  [4.8260870e+02 5.2804346e+02 2.0107470e-02],  [4.6108698e+02 5.3043475e+02 2.2252293e-02],  [4.7543481e+02 5.6391302e+02 2.0914197e-02],  [4.7543481e+02 5.2804346e+02 2.7845293e-02],  [4.6586960e+02 5.4000000e+02 2.5432626e-02],  [5.1847827e+02 5.4956519e+02 1.6933031e-02],  [4.8021741e+02 5.6391302e+02 1.2440545e-02],  [5.3282611e+02 4.5630432e+02 1.2662101e-02],  [5.0652176e+02 5.4000000e+02 2.5466656e-02],  [5.1130435e+02 5.2565216e+02 2.5591202e-02],  [5.0891306e+02 5.4956519e+02 1.0660242e-02],  [4.7065219e+02 5.2565216e+02 1.6639609e-02],  [5.2804352e+02 5.0173911e+02 2.2738149e-02],  [5.0413046e+02 5.3043475e+02 2.0388147e-02],  [4.8021741e+02 5.5434778e+02 1.0172251e-02],  [4.5152176e+02 4.3239130e+02 1.6042734e-02],  [4.8500003e+02 5.4956519e+02 1.4421927e-02],  [4.9217392e+02 5.6152173e+02 1.5104427e-02],  [4.8978262e+02 5.6152173e+02 1.0827912e-02]]]\",\"[[[4.8782608e+02 4.5434781e+02 4.6337242e-03],  [4.8065216e+02 4.9021738e+02 8.7779146e-03],  [4.8782608e+02 5.2847821e+02 8.5121123e-03],  [4.7108694e+02 4.6869565e+02 8.6766174e-03],  [4.6869565e+02 4.6630432e+02 7.1836482e-03],  [5.4043475e+02 4.2565216e+02 8.7995632e-03],  [4.2804346e+02 4.8782608e+02 8.3582215e-03],  [3.4913043e+02 5.1173911e+02 1.0578417e-02],  [4.0891302e+02 4.8782608e+02 9.7930310e-03],  [5.2847821e+02 4.1130432e+02 5.6122812e-03],  [4.2804346e+02 4.8782608e+02 8.1787128e-03],  [4.3043475e+02 4.9499997e+02 1.0875644e-02],  [4.1608694e+02 4.9021738e+02 9.1631478e-03],  [4.1608694e+02 4.8782608e+02 8.3672330e-03],  [4.2804346e+02 4.8782608e+02 9.0226326e-03],  [4.2565216e+02 5.0217389e+02 9.3891509e-03],  [4.1847824e+02 5.0934781e+02 8.6548291e-03],  [5.3086957e+02 4.2565216e+02 1.0162307e-02],  [3.5391302e+02 5.2369562e+02 1.0465117e-02],  [3.5391302e+02 5.2369562e+02 1.5369176e-02],  [3.6347824e+02 5.2130432e+02 8.9091975e-03]]]\"')\n",
        "\n",
        "# print(v_id, img_id)\n",
        "# print(hand)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohetAJfDhcbL"
      },
      "source": [
        "import random\n",
        "import datetime\n",
        "\n",
        "def load_data():\n",
        "  \"\"\" Load data from the csv files.\"\"\"\n",
        "  result_dir = Path(root) / 'OpenPose.csv'\n",
        "  csv_list = ['result_Gesture1.csv','result_Gesture2.csv','result_Gesture3.csv','result_Gesture4.csv','result_Gesture5.csv']\n",
        "  train = []\n",
        "  test = []\n",
        "  for i in range(5):\n",
        "    print(\"reading start for\", i, \"th csv!\", datetime.datetime.now().time())\n",
        "    result_dir = Path(root) / csv_list[i]\n",
        "    data_csv = open(str(result_dir), 'r')\n",
        "    data_lines = data_csv.readlines()[1:]\n",
        "\n",
        "    dataset =  dict() # create empty dictionary\n",
        "    v_ids = set()\n",
        "    \n",
        "    for line in data_lines: \n",
        "      v_id, img_id, hand = parser(line) # parse each data line. l, r contains joint position of a image.\n",
        "\n",
        "      if v_id in v_ids: # if v_id already exists, append the joints to existing video id.\n",
        "        dataset[v_id].append(hand)\n",
        "      else: # if not, start new sequence.\n",
        "        dataset[v_id] = [hand]\n",
        "        v_ids.add(v_id)\n",
        "    tot = list(dataset.values())\n",
        "    totlen = len(tot)\n",
        "    lens = [len(v) for v in dataset.values()]\n",
        "    print(set(lens))\n",
        "    for v in tot[:int(totlen*0.8)]:\n",
        "      # x = cut_n(torch.stack(v), 12)\n",
        "      x = resample_n(torch.stack(v), 100)\n",
        "      if x is None:\n",
        "        continue\n",
        "      # x_ = torch.stack([x[i+1] - x[i] for i in range(x.shape[0]-1)])\n",
        "      # x = torch.cat((x, x_))\n",
        "      train.append((x, i))\n",
        "\n",
        "    for v in tot[int(totlen*0.8):]:\n",
        "      # x = cut_n(torch.stack(v), 12)\n",
        "      x = resample_n(torch.stack(v), 100)\n",
        "      if x is None:\n",
        "        continue\n",
        "      # x_ = torch.stack([x[i+1] - x[i] for i in range(x.shape[0]-1)])\n",
        "      # x = torch.cat((x, x_))\n",
        "      test.append((x, i))\n",
        "\n",
        "  random.shuffle(train)\n",
        "  random.shuffle(test)\n",
        "\n",
        "  return train, test\n",
        "\n",
        "train, test = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb3NnGhqhf65"
      },
      "source": [
        "class HandposeDataset(Dataset):\n",
        "  \"\"\" handpose dataset \"\"\"\n",
        "  def __init__(self, dataset):\n",
        "    self.len = len(dataset)\n",
        "    # print(dataset[0][0].size(), dataset[0][0][:,:,:-1].size())\n",
        "    print(dataset[0][0].shape)\n",
        "    x = [i[0][:,:,:-1] for i in dataset] # without confidence\n",
        "    print(x[0].size())\n",
        "    # x = [i[0].view(i[0].size()[0], -1) for i in dataset] # with confidence\n",
        "    print(len(x), \"len of x\")\n",
        "    x_data = torch.stack(x)\n",
        "    self.x_data = x_data.view(x_data.size()[0], x_data.size()[1], -1)\n",
        "    y = [i[1] for i in dataset]\n",
        "    print(len(y), \"len of y\")\n",
        "    self.y_data = torch.tensor(y)\n",
        "    print('x_data', self.x_data.shape)\n",
        "    print('y_data', self.y_data.shape)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSbW2J3AYawG"
      },
      "source": [
        "hand_train = HandposeDataset(dataset = train)\n",
        "hand_test = HandposeDataset(dataset = test)\n",
        "\n",
        "train_loader = DataLoader(dataset = hand_train,\n",
        "                          batch_size=32,\n",
        "                          shuffle = True,\n",
        "                          num_workers = 4)\n",
        "test_loader = DataLoader(dataset = hand_test,\n",
        "                          batch_size=32,\n",
        "                          shuffle = True,\n",
        "                          num_workers = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WJg_BK8PUhd"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6V3Q6PTYjPf"
      },
      "source": [
        "n_classes = 5\n",
        "duration = 100\n",
        "n_channels = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTqC9q7HPVno"
      },
      "source": [
        "class HandGestureNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    [Devineau et al., 2018] Deep Learning for Hand Gesture Recognition on Skeletal Data\n",
        "\n",
        "    Summary\n",
        "    -------\n",
        "        Deep Learning Model for Hand Gesture classification using pose data only (no need for RGBD)\n",
        "        The model computes a succession of [convolutions and pooling] over time independently on each of the 66 (= 22 * 3) sequence channels.\n",
        "        Each of these computations are actually done at two different resolutions, that are later merged by concatenation\n",
        "        with the (pooled) original sequence channel.\n",
        "        Finally, a multi-layer perceptron merges all of the processed channels and outputs a classification.\n",
        "    \n",
        "    TL;DR:\n",
        "    ------\n",
        "        input ------------------------------------------------> split into n_channels channels [channel_i]\n",
        "            channel_i ----------------------------------------> 3x [conv/pool/dropout] low_resolution_i\n",
        "            channel_i ----------------------------------------> 3x [conv/pool/dropout] high_resolution_i\n",
        "            channel_i ----------------------------------------> pooled_i\n",
        "            low_resolution_i, high_resolution_i, pooled_i ----> output_channel_i\n",
        "        MLP(n_channels x [output_channel_i]) -------------------------> classification\n",
        "\n",
        "    Article / PDF:\n",
        "    --------------\n",
        "        https://ieeexplore.ieee.org/document/8373818\n",
        "\n",
        "    Please cite:\n",
        "    ------------\n",
        "        @inproceedings{devineau2018deep,\n",
        "            title={Deep learning for hand gesture recognition on skeletal data},\n",
        "            author={Devineau, Guillaume and Moutarde, Fabien and Xi, Wang and Yang, Jie},\n",
        "            booktitle={2018 13th IEEE International Conference on Automatic Face \\& Gesture Recognition (FG 2018)},\n",
        "            pages={106--113},\n",
        "            year={2018},\n",
        "            organization={IEEE}\n",
        "        }\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_channels=42, n_classes=5, dropout_probability=0.2):\n",
        "\n",
        "        super(HandGestureNet, self).__init__()\n",
        "        \n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_probability = dropout_probability\n",
        "\n",
        "        # Layers ----------------------------------------------\n",
        "        self.all_conv_high = torch.nn.ModuleList([torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=7, padding=3),\n",
        "            # torch.nn.ReLU(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=7, padding=3),\n",
        "            # torch.nn.ReLU(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=7, padding=3),\n",
        "            # torch.nn.ReLU(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout(p=self.dropout_probability),\n",
        "            torch.nn.AvgPool1d(2)\n",
        "        ) for joint in range(n_channels)])\n",
        "\n",
        "        self.all_conv_low = torch.nn.ModuleList([torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n",
        "            # torch.nn.ReLU(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, padding=1),\n",
        "            # torch.nn.ReLU(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels=4, out_channels=4, kernel_size=3, padding=1),\n",
        "            # torch.nn.ReLU(),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Dropout(p=self.dropout_probability),\n",
        "            torch.nn.AvgPool1d(2)\n",
        "        ) for joint in range(n_channels)])\n",
        "\n",
        "        self.all_residual = torch.nn.ModuleList([torch.nn.Sequential(\n",
        "            torch.nn.AvgPool1d(2),\n",
        "            torch.nn.AvgPool1d(2),\n",
        "            torch.nn.AvgPool1d(2)\n",
        "        ) for joint in range(n_channels)])\n",
        "\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=9 * n_channels*12, out_features=1936),  # <-- 12: depends of the sequences lengths (cf. below)\n",
        "            # torch.nn.Linear(in_features=9 * n_channels, out_features=1936),  # <-- 12: depends of the sequences lengths (cf. below)\n",
        "            torch.nn.ReLU(),\n",
        "            # torch.nn.LeakyReLU(),\n",
        "            torch.nn.Linear(in_features=1936, out_features=n_classes)\n",
        "        )\n",
        "\n",
        "        # Initialization --------------------------------------\n",
        "        # Xavier init\n",
        "        for module in itertools.chain(self.all_conv_high, self.all_conv_low, self.all_residual):\n",
        "            for layer in module:\n",
        "                if layer.__class__.__name__ == \"Conv1d\":\n",
        "                    torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "                    torch.nn.init.constant_(layer.bias, 0.1)\n",
        "\n",
        "        for layer in self.fc:\n",
        "            if layer.__class__.__name__ == \"Linear\":\n",
        "                torch.nn.init.xavier_uniform_(layer.weight, gain=torch.nn.init.calculate_gain('relu'))\n",
        "                torch.nn.init.constant_(layer.bias, 0.1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        This function performs the actual computations of the network for a forward pass.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "            input: a tensor of gestures of shape (batch_size, duration, n_channels)\n",
        "                   (where n_channels = 3 * n_joints for 3D pose data)\n",
        "        \"\"\"\n",
        "\n",
        "        # Work on each channel separately\n",
        "        all_features = []\n",
        "\n",
        "        for channel in range(0, self.n_channels):\n",
        "            input_channel = input[:, :, channel]\n",
        "\n",
        "            # Add a dummy (spatial) dimension for the time convolutions\n",
        "            # Conv1D format : (batch_size, n_feature_maps, duration)\n",
        "            input_channel = input_channel.unsqueeze(1)\n",
        "\n",
        "            high = self.all_conv_high[channel](input_channel)\n",
        "            low = self.all_conv_low[channel](input_channel)\n",
        "            ap_residual = self.all_residual[channel](input_channel)\n",
        "\n",
        "            # Time convolutions are concatenated along the feature maps axis\n",
        "            output_channel = torch.cat([\n",
        "                high,\n",
        "                low,\n",
        "                ap_residual\n",
        "            ], dim=1)\n",
        "            all_features.append(output_channel)\n",
        "\n",
        "        # Concatenate along the feature maps axis\n",
        "        all_features = torch.cat(all_features, dim=1)\n",
        "        # Flatten for the Linear layers\n",
        "        all_features = all_features.view(-1, 9 * self.n_channels*12)  # <-- 12: depends of the initial sequence length (100).\n",
        "        # all_features = all_features.view(-1, 9 * self.n_channels)\n",
        "        # If you have shorter/longer sequences, you probably do NOT even need to modify the modify the network architecture:\n",
        "        # resampling your input gesture from T timesteps to 100 timesteps will (surprisingly) probably actually work as well!\n",
        "\n",
        "        # Fully-Connected Layers\n",
        "        output = self.fc(all_features)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_9SGFHzYtG_"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLWgtKSQwIoy"
      },
      "source": [
        "def train_model(model, criterion, optimizer, trainloader, testloader, num_epochs=5, model_name='hello'):\n",
        "    # Training starting time\n",
        "    global_step = 0\n",
        "    best_accuracy = 0\n",
        "    start = time.time()\n",
        "    ckpt_dir = Path(root)\n",
        "    print('[INFO] Started to train the model.')\n",
        "    print('Training the model on {}.'.format('GPU' if device == torch.device('cuda') else 'CPU'))\n",
        "    for ep in range(num_epochs):\n",
        "        model.train()\n",
        "        current_loss = 0.0\n",
        "        for idx_batch, batch in enumerate(trainloader):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward() \n",
        "            optimizer.step()\n",
        "            \n",
        "            current_loss += loss.item()\n",
        "        print('train loss:', current_loss)\n",
        "        model.eval()\n",
        "\n",
        "        nb_classes = 5\n",
        "        confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0.\n",
        "            test_accuracy = 0.\n",
        "            test_num_data = 0.\n",
        "            for batch_idx, (x, y) in enumerate(testloader):\n",
        "                x = x.to(device=device)\n",
        "                y = y.to(device=device)\n",
        "                \n",
        "                logit = model(x)\n",
        "                pred = logit.argmax(dim = 1)\n",
        "                accuracy = (pred == y).float().mean()\n",
        "                loss = criterion(logit, y)\n",
        "\n",
        "                test_loss += loss.item()*x.shape[0]\n",
        "                test_accuracy += accuracy.item()*x.shape[0]\n",
        "                test_num_data += x.shape[0]\n",
        "                \n",
        "                for t, p in zip(y.view(-1), pred.view(-1)):\n",
        "                  confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "            test_loss /= test_num_data\n",
        "            test_accuracy /= test_num_data\n",
        "            print('epoch:', ep, 'test loss:', test_loss, 'test_accuracy:', test_accuracy)\n",
        "            # print(confusion_matrix)\n",
        "            if test_accuracy > best_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_confusion = confusion_matrix\n",
        "                torch.save(model.state_dict(), f'{ckpt_dir}/{model_name}.pt')\n",
        "        scheduler.step()\n",
        "    return best_confusion, best_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb4PTmmjwIo4",
        "collapsed": true
      },
      "source": [
        "model = HandGestureNet(n_channels=42, n_classes=5)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0002)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma = 0.5)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "confusion, accuracy = train_model(model=model, criterion=criterion, optimizer=optimizer, \n",
        "                                  trainloader=train_loader, testloader = test_loader, num_epochs=num_epochs, model_name='cnn_mid12')\n",
        "\n",
        "print(f'Best test accuracy: {accuracy:.3f}')\n",
        "print(confusion)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}