{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GestureMLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrlCJ-71eRwS"
      },
      "source": [
        "# Hand Gesture Detector with MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OhhALZUymNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51b0da9-f2e2-4e65-9a4e-c00a7815b273"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNEH-RfbysJT"
      },
      "source": [
        "root = '/gdrive/My Drive/MotionDetection'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67imU-PQ9R7"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcG3lW4fYhYz"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# set manual seeds \n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExTFuTWmRGr-"
      },
      "source": [
        "## Functions for Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QqeeFkUqI9e"
      },
      "source": [
        "def normalize(tens):\n",
        "  \"\"\" Normalize 4 dimensional Tensor: batch_size * duration * number of joints * xy-coordinate\n",
        "      Maintain the ratio and make sure the joints are in 100*100 box.\"\"\"\n",
        "  res = torch.zeros(tens.shape)\n",
        "  for i in range(tens.shape[0]):\n",
        "    for j in range(tens.shape[1]):\n",
        "      temp = tens[i, j]\n",
        "      # print(temp)\n",
        "      max_x = max(temp[:, 0])\n",
        "      min_x = min(temp[:, 0])\n",
        "      max_y = max(temp[:, 1])\n",
        "      min_y = min(temp[:, 1])\n",
        "      ratio = (max_x-min_x) / (max_y - min_y)\n",
        "      res[i, j, :, 0] = (temp[:, 0] - min_x + 1) / (max_y - min_y) * 100\n",
        "      res[i, j, :, 1] = (temp[:, 1] - min_y + 1) / (max_y - min_y) * 100\n",
        "      # print(res[i, j])\n",
        "  return res\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# test = torch.rand(1, 2, 21, 2)\n",
        "# test[:, :, :, 0] = 12 * test[:, :, :, 0]\n",
        "# test[:, :, :, 1] = 10 * test[:, :, :, 1]\n",
        "\n",
        "# test1 = normalize(test)\n",
        "# plt.scatter(test[:,0, :, 0], test[:, 0, :, 1])\n",
        "# plt.show()\n",
        "# plt.scatter(test1[:,0, :, 0], test1[:, 0, :, 1])\n",
        "# plt.show()\n",
        "# plt.scatter(test[:,1, :, 0], test[:, 1, :, 1])\n",
        "# plt.show()\n",
        "# plt.scatter(test1[:,1, :, 0], test1[:, 1, :, 1])\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbIi-70_nEU6"
      },
      "source": [
        "def randn_ordered(tens, n):\n",
        "  \"\"\"Random sample n from a tensor, maintaining the order\"\"\"\n",
        "  assert tens.size(0) > n\n",
        "  print(type(tens.shape[0]))\n",
        "  r = random.sample(range(tens.shape[0]), n)\n",
        "  r.sort()\n",
        "  \n",
        "  return tens[r]\n",
        "  \n",
        "# random.seed(a=123)\n",
        "# test = torch.arange(3*4*5)\n",
        "# test = test.reshape(5, 4, 3)\n",
        "# \n",
        "# # print(test)\n",
        "# # print(randn_ordered(test, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO8GI0fRM3VU"
      },
      "source": [
        "def cut_n(tens, n):\n",
        "  \"\"\"cut n frames in the middle(0.6) from the sequence\"\"\"\n",
        "  if tens.shape[0] == n:\n",
        "    return tens\n",
        "\n",
        "  tt = int(tens.shape[0] * 0.6) - 1\n",
        "  c = tens[tt-n//2:tt+(n-n//2)]\n",
        "  if c.shape[0] != n:\n",
        "    return None\n",
        "  return tens[tt-n//2:tt+(n-n//2)]\n",
        "\n",
        "# test = torch.arange(10*5*3)\n",
        "# test = test.reshape(10, 5, 3)\n",
        "# \n",
        "# print(test)\n",
        "# print(cut_n(test, 2))\n",
        "# test2= torch.tensor([[5.8782611e+02, 5.1130432e+02, 3.5325505e-02],  [5.8304352e+02, 4.3956522e+02, 2.4411943e-02],  [4.8260870e+02, 5.2804346e+02, 2.0107470e-02]])\n",
        "# print(cut_n(test2,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLo3bDl5KlHf"
      },
      "source": [
        "def cut_max2(tens):\n",
        "  \"\"\" Find maximum confidence frame and return two frames(maximum and one of its neighbor)\"\"\"\n",
        "  # print(torch.sum(tens[:, :, -1], dim=1))\n",
        "  tt = torch.argmax(torch.sum(tens[:, :, -1], dim=1), dim=0)\n",
        "  # return torch.unsqueeze(tens[tt], 0)\n",
        "  # print(tt)\n",
        "  if tt == 0:\n",
        "    return tens[:2]\n",
        "  if tt == tens.shape[0]-1:\n",
        "    return tens[-2:]\n",
        "  if torch.sum(tens[tt-1, :, -1]) >= torch.sum(tens[tt+1, :, -1]):\n",
        "    return tens[tt-1:tt+1]\n",
        "  return tens[tt:tt+2]\n",
        "\n",
        "# test = torch.rand(5*21*3)\n",
        "# test = test.reshape(5, 21, 3)\n",
        "\n",
        "\n",
        "# print(test)\n",
        "# print(cut_max2(test))\n",
        "# test2= torch.tensor([[5.8782611e+02, 5.1130432e+02, 3.5325505e-02],  [5.8304352e+02, 4.3956522e+02, 2.4411943e-02],  [4.8260870e+02, 5.2804346e+02, 2.0107470e-02]])\n",
        "# print(cut_max2(test2,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4LJ2Q9y-8UK"
      },
      "source": [
        "def cut_max3(tens):\n",
        "  \"\"\" Find maximum confidence frame and return two frames(maximum and two of its neighbor)\"\"\"\n",
        "  # print(torch.sum(tens[:, :, -1], dim=1))\n",
        "  tt = torch.argmax(torch.sum(tens[:, :, -1], dim=1), dim=0)\n",
        "  if tt == 0:\n",
        "    return tens[:3]\n",
        "  if tt == tens.shape[0]-1:\n",
        "    return tens[-3:]\n",
        "  return tens[tt-1:tt+2]\n",
        "# test = torch.rand(5*21*3)\n",
        "# test = test.reshape(5, 21, 3)\n",
        "\n",
        "\n",
        "# print(test)\n",
        "# print(cut_max2(test))\n",
        "# test2= torch.tensor([[5.8782611e+02, 5.1130432e+02, 3.5325505e-02],  [5.8304352e+02, 4.3956522e+02, 2.4411943e-02],  [4.8260870e+02, 5.2804346e+02, 2.0107470e-02]])\n",
        "# print(cut_max2(test2,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9_zS7nLRyvl"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTkjNG8gd3P5"
      },
      "source": [
        "def parser(line):\n",
        "  \"\"\" Our data is in csv format. Parse each line of the csv\"\"\"\n",
        "  id, left, right = line.split('[[[')\n",
        "  id = id[:-2]\n",
        "  left = '[' + left[:-4]\n",
        "  right = '[' + right[:-2]\n",
        "\n",
        "  left = left.split(',  ')\n",
        "  left = [i.strip('[]').split(' ') for i in left]\n",
        "  left = [list(filter(None, leftelem)) for leftelem in left]\n",
        "  left = [float(i) for j in left for i in j]\n",
        "  left = torch.tensor(left)\n",
        "  left = left.reshape((-1, 3))\n",
        "  left_avg = sum(left[:, 2])/21\n",
        "  \n",
        "  right = right.split(',  ')\n",
        "  right = [i.strip('[]').split(' ') for i in right]\n",
        "  right = [list(filter(None, rightelem)) for rightelem in right]\n",
        "  right = [float(i) for j in right for i in j]\n",
        "  right = torch.tensor(right)\n",
        "  right = left.reshape((-1, 3))\n",
        "  right_avg = sum(right[:, 2])/21\n",
        "\n",
        "  v_id, img_id = id.split('_')\n",
        "\n",
        "  if left_avg >= right_avg:\n",
        "    left[:,0] = -left[:,0]\n",
        "    # return v_id, img_id, normalize(left)\n",
        "    return v_id, img_id, left\n",
        "  # return v_id, img_id, normalize(right)\n",
        "  return v_id, img_id, right\n",
        "  #   return v_id, img_id, normalize(left[:, :2])\n",
        "  # return v_id, img_id, normalize(right[:, :2])\n",
        "\n",
        "\n",
        "# v_id, img_id, hand = parser('1_00001,\"[[[5.8782611e+02 5.1130432e+02 3.5325505e-02],  [5.8304352e+02 4.3956522e+02 2.4411943e-02],  [4.8260870e+02 5.2804346e+02 2.0107470e-02],  [4.6108698e+02 5.3043475e+02 2.2252293e-02],  [4.7543481e+02 5.6391302e+02 2.0914197e-02],  [4.7543481e+02 5.2804346e+02 2.7845293e-02],  [4.6586960e+02 5.4000000e+02 2.5432626e-02],  [5.1847827e+02 5.4956519e+02 1.6933031e-02],  [4.8021741e+02 5.6391302e+02 1.2440545e-02],  [5.3282611e+02 4.5630432e+02 1.2662101e-02],  [5.0652176e+02 5.4000000e+02 2.5466656e-02],  [5.1130435e+02 5.2565216e+02 2.5591202e-02],  [5.0891306e+02 5.4956519e+02 1.0660242e-02],  [4.7065219e+02 5.2565216e+02 1.6639609e-02],  [5.2804352e+02 5.0173911e+02 2.2738149e-02],  [5.0413046e+02 5.3043475e+02 2.0388147e-02],  [4.8021741e+02 5.5434778e+02 1.0172251e-02],  [4.5152176e+02 4.3239130e+02 1.6042734e-02],  [4.8500003e+02 5.4956519e+02 1.4421927e-02],  [4.9217392e+02 5.6152173e+02 1.5104427e-02],  [4.8978262e+02 5.6152173e+02 1.0827912e-02]]]\",\"[[[4.8782608e+02 4.5434781e+02 4.6337242e-03],  [4.8065216e+02 4.9021738e+02 8.7779146e-03],  [4.8782608e+02 5.2847821e+02 8.5121123e-03],  [4.7108694e+02 4.6869565e+02 8.6766174e-03],  [4.6869565e+02 4.6630432e+02 7.1836482e-03],  [5.4043475e+02 4.2565216e+02 8.7995632e-03],  [4.2804346e+02 4.8782608e+02 8.3582215e-03],  [3.4913043e+02 5.1173911e+02 1.0578417e-02],  [4.0891302e+02 4.8782608e+02 9.7930310e-03],  [5.2847821e+02 4.1130432e+02 5.6122812e-03],  [4.2804346e+02 4.8782608e+02 8.1787128e-03],  [4.3043475e+02 4.9499997e+02 1.0875644e-02],  [4.1608694e+02 4.9021738e+02 9.1631478e-03],  [4.1608694e+02 4.8782608e+02 8.3672330e-03],  [4.2804346e+02 4.8782608e+02 9.0226326e-03],  [4.2565216e+02 5.0217389e+02 9.3891509e-03],  [4.1847824e+02 5.0934781e+02 8.6548291e-03],  [5.3086957e+02 4.2565216e+02 1.0162307e-02],  [3.5391302e+02 5.2369562e+02 1.0465117e-02],  [3.5391302e+02 5.2369562e+02 1.5369176e-02],  [3.6347824e+02 5.2130432e+02 8.9091975e-03]]]\"')\n",
        "\n",
        "# print(v_id, img_id)\n",
        "# print(hand)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHfLQgcYxErf"
      },
      "source": [
        "import random\n",
        "import datetime\n",
        "\n",
        "def load_data():\n",
        "  \"\"\" Load data from the csv files.\"\"\"\n",
        "  result_dir = Path(root) / 'OpenPose.csv'\n",
        "  csv_list = ['result_Gesture1.csv','result_Gesture2.csv','result_Gesture3.csv','result_Gesture4.csv','result_Gesture5.csv']\n",
        "  train = []\n",
        "  test = []\n",
        "  for i in range(5):\n",
        "    print(\"reading start for\", i, \"th csv!\", datetime.datetime.now().time())\n",
        "    result_dir = Path(root) / csv_list[i]\n",
        "    data_csv = open(str(result_dir), 'r')\n",
        "    data_lines = data_csv.readlines()[1:]\n",
        "\n",
        "    dataset =  dict() # create empty dictionary\n",
        "    v_ids = set()\n",
        "    \n",
        "    for line in data_lines: \n",
        "      v_id, img_id, hand = parser(line) # parse each data line. l, r contains joint position of a image.\n",
        "\n",
        "      if v_id in v_ids: # if v_id already exists, append the joints to existing video id.\n",
        "        dataset[v_id].append(hand)\n",
        "      else: # if not, start new sequence.\n",
        "        dataset[v_id] = [hand]\n",
        "        v_ids.add(v_id)\n",
        "    tot = list(dataset.values())\n",
        "    totlen = len(tot)\n",
        "    \n",
        "    for v in tot[:int(totlen*0.8)]:\n",
        "      x = cut_max2(torch.stack(v))\n",
        "      if x is None:\n",
        "        continue\n",
        "      # x_ = torch.stack([x[i+1] - x[i] for i in range(x.shape[0]-1)])\n",
        "      # x = torch.cat((x, x_))\n",
        "      train.append((x, i))\n",
        "\n",
        "    for v in tot[int(totlen*0.8):]:\n",
        "      x = cut_max2(torch.stack(v))\n",
        "      if x is None:\n",
        "        continue\n",
        "      # x_ = torch.stack([x[i+1] - x[i] for i in range(x.shape[0]-1)])\n",
        "      # x = torch.cat((x, x_))\n",
        "      test.append((x, i))\n",
        "\n",
        "  random.shuffle(train)\n",
        "  random.shuffle(test)\n",
        "\n",
        "  return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAPaHhZ2LMei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890cae14-164f-494b-c4fe-08cc4a0647f1"
      },
      "source": [
        "train, test = load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading start for 0 th csv! 12:17:39.387453\n",
            "reading start for 1 th csv! 12:19:01.386062\n",
            "reading start for 2 th csv! 12:20:14.721937\n",
            "reading start for 3 th csv! 12:21:31.964052\n",
            "reading start for 4 th csv! 12:22:51.346956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmFlCTA0u-4W",
        "outputId": "06caaca3-b225-47f2-e859-98c087692020"
      },
      "source": [
        "print(len(test), len(train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4462 17838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghgBx331LMlZ"
      },
      "source": [
        "class HandposeDataset(Dataset):\n",
        "  \"\"\" handpose dataset \"\"\"\n",
        "  def __init__(self, dataset):\n",
        "    # temp = []\n",
        "    # for i in dataset:\n",
        "    #   if i[1] != 0:\n",
        "    #     temp.append((i[0], i[1]-1))\n",
        "    # dataset = temp\n",
        "    self.len = len(dataset)\n",
        "    \n",
        "    # x = [i[0][:,:,:-1] for i in dataset] # without confidence\n",
        "    x = [i[0] for i in dataset] # with confidence\n",
        "    print(len(x), \"len of x\")\n",
        "    self.x_data = torch.stack(x)\n",
        "    y = [i[1] for i in dataset]\n",
        "    # for i, v in enumerate(y):\n",
        "      # if v == 3:\n",
        "      #   y[i] = 0\n",
        "      # elif v == 4:\n",
        "      #   y[i] = 3\n",
        "    print(len(y), \"len of y\")\n",
        "    self.y_data = torch.tensor(y)\n",
        "    print('x_data', self.x_data.shape)\n",
        "    print('y_data', self.y_data.shape)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "hand_train = HandposeDataset(dataset = train)\n",
        "hand_test = HandposeDataset(dataset = test)\n",
        "\n",
        "train_loader = DataLoader(dataset = hand_train,\n",
        "                          batch_size=32,\n",
        "                          shuffle = True,\n",
        "                          num_workers = 1)\n",
        "test_loader = DataLoader(dataset = hand_test,\n",
        "                          batch_size=32,\n",
        "                          shuffle = False,\n",
        "                          num_workers = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISR2_tINWnNi"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiEirBnJeYCQ"
      },
      "source": [
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(MLPBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_channels, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512,256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 64)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        # self.dropout = nn.Dropout(p=0.2)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.act(self.bn1(self.fc1(x)))\n",
        "        output = self.act(self.bn2(self.fc2(output)))\n",
        "        output = self.act(self.bn3(self.fc3(output)))\n",
        "\n",
        "        # output = self.act(self.fc1(x))\n",
        "        # output = self.act(self.fc2(output))\n",
        "        # output = self.act(self.fc3(output))\n",
        "        # output = self.act(self.fc3(output))\n",
        "        \n",
        "        # output = self.dropout(self.act(self.bn1(self.fc1(x))))\n",
        "        # output = self.dropout(self.act(self.bn2(self.fc2(output))))\n",
        "        # output = self.dropout(self.act(self.bn3(self.fc3(output))))\n",
        "        # output = self.dropout(self.act(self.bn3(self.fc3(output))))\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-38BGWdWhMx"
      },
      "source": [
        "class GestureDetector(nn.Module):\n",
        "    def __init__(self, nf):\n",
        "          super(GestureDetector, self).__init__()\n",
        "          block = MLPBlock\n",
        "          self.mlp = block(2*21*3, nf) # with confidence\n",
        "          # self.mlp = block(2*21*2, nf) # without confidence\n",
        "          self.fc = nn.Linear(nf, 5) # our gesture dataset is consisted of 5 classes\n",
        "          # self.fc = nn.Linear(nf, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = normalize(x)\n",
        "        output = self.mlp(x.view(x.size()[0], -1))\n",
        "        output = self.fc(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGe1LaVDWjZ7"
      },
      "source": [
        "def weight_init(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.constant_(m.weight, 1)\n",
        "        torch.nn.init.constant_(m.bias, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9rLt151WsqS"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sTYJzW7Hcr2"
      },
      "source": [
        "def train_net(net, optimizer, scheduler, model_name, epoch):\n",
        "    global_step = 0\n",
        "    best_accuracy = 0\n",
        "    ckpt_dir = Path(root)\n",
        "    for epoch in range(epoch):\n",
        "        net.train()\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            global_step += 1\n",
        "\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)            \n",
        "\n",
        "            logit = net(x)\n",
        "            pred = logit.argmax(dim = 1)\n",
        "            accuracy = (pred == y).float().mean()\n",
        "            loss = nn.CrossEntropyLoss()(logit, y)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # test loop.\n",
        "        net.eval()\n",
        "        nb_classes = 5\n",
        "        confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "        with torch.no_grad():\n",
        "            test_loss = 0.\n",
        "            test_accuracy = 0.\n",
        "            test_num_data = 0.\n",
        "            for batch_idx, (x, y) in enumerate(test_loader):\n",
        "                x = x.to(device=device)\n",
        "                y = y.to(device=device)\n",
        "                \n",
        "                logit = net(x)\n",
        "                pred = logit.argmax(dim = 1)\n",
        "                accuracy = (pred == y).float().mean()\n",
        "                loss = nn.CrossEntropyLoss()(logit, y)\n",
        "\n",
        "                test_loss += loss.item()*x.shape[0]\n",
        "                test_accuracy += accuracy.item()*x.shape[0]\n",
        "                test_num_data += x.shape[0]\n",
        "                \n",
        "                for t, p in zip(y.view(-1), pred.view(-1)):\n",
        "                  confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "            test_loss /= test_num_data\n",
        "            test_accuracy /= test_num_data\n",
        "            print('epoch:', epoch, 'test loss:', test_loss, 'test_accuracy:', test_accuracy)\n",
        "            if test_accuracy > best_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_confusion = confusion_matrix\n",
        "                torch.save(net.state_dict(), f'{ckpt_dir}/{model_name}.pt')\n",
        "        scheduler.step()\n",
        "    return best_confusion, best_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kEM8BlWCSAV"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "network = GestureDetector(64).to(device)\n",
        "network.apply(weight_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b4dm9I5D6f4"
      },
      "source": [
        "final_accs = {}\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(network.parameters(), lr=0.0005, momentum=0.9, weight_decay=0.0001)\n",
        "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,80], gamma=0.5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma = 0.5)\n",
        "epoch = 100\n",
        "\n",
        "global_step = 0\n",
        "best_accuracy = 0.0\n",
        "\n",
        "t1 = time.time()\n",
        "confusion, accuracy = train_net(network, optimizer, scheduler, 'max2_4l', epoch)\n",
        "t = time.time()-t1\n",
        "print(f'Best test accuracy of {block_type} network : {accuracy:.3f} took {t:.3f} secs')\n",
        "print(confusion)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}